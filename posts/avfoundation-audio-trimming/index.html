<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.99.1" />



<title>Trimming Audio Files and Adding Silence with AVFoundation in Swift - Artem Garmash</title>


<meta name="author" content="Artem Garmash" />



<meta name="keywords" content="Swift, AVFoundation" />

<meta property="og:title" content="Trimming Audio Files and Adding Silence with AVFoundation in Swift" />
<meta property="og:description" content="
Originally posted in Akvelon Blog

While working on one of the apps, we faced the need to trim the recorded audio files. We were working with 32-bit float WAV files, and we had the following requirements:

the output file should have the exact same format as the input file;
no processing should be applied to the audio data, audio samples should be copied as-is;
there should be an ability to add silence to the output file.

My first guess was to use AVAssetExportSession, but it has limited options for exporting the audio, and it’s not possible to be sure what it does with the audio under the hood. A no-go.
Secondly, I took a look at the requirements again. &ldquo;Audio samples should be copied as-is&rdquo;. That was exactly what we needed - to open the input file for reading, the output one for writing, calculate the range of audio samples to copy, and perform the actual copying. Fortunately, it was completely possible with AVAudioFile - it can be read into AVAudioPCMBuffer and written from one&rsquo;s contents." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://agarmash.com/posts/avfoundation-audio-trimming/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-26T13:39:08+01:00" />
<meta property="article:modified_time" content="2021-12-26T13:39:08+01:00" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Trimming Audio Files and Adding Silence with AVFoundation in Swift"/>
<meta name="twitter:description" content="
Originally posted in Akvelon Blog

While working on one of the apps, we faced the need to trim the recorded audio files. We were working with 32-bit float WAV files, and we had the following requirements:

the output file should have the exact same format as the input file;
no processing should be applied to the audio data, audio samples should be copied as-is;
there should be an ability to add silence to the output file.

My first guess was to use AVAssetExportSession, but it has limited options for exporting the audio, and it’s not possible to be sure what it does with the audio under the hood. A no-go.
Secondly, I took a look at the requirements again. &ldquo;Audio samples should be copied as-is&rdquo;. That was exactly what we needed - to open the input file for reading, the output one for writing, calculate the range of audio samples to copy, and perform the actual copying. Fortunately, it was completely possible with AVAudioFile - it can be read into AVAudioPCMBuffer and written from one&rsquo;s contents."/>





<link rel="stylesheet" href="https://agarmash.com/assets/css/fuji.min.css" />





<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-179143853-1');
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-179143853-1"></script>


</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://agarmash.com">Artem Garmash</a>
            
            <span class="title-sub">Software developer, hardware enthusiast, mountain bike rider, musichead</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://agarmash.com/posts/avfoundation-audio-trimming/">Trimming Audio Files and Adding Silence with AVFoundation in Swift</a>
    </h2>
    
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-12-26</span><span><i class="iconfont icon-time-sharp"></i>&nbsp;9 minutes</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/swift">Swift</a>&nbsp;<a href="/tags/avfoundation">AVFoundation</a>&nbsp;</span>

    </div>
    
    
    
    <div class="post-content markdown-body">
        <blockquote>
<p>Originally posted in <a href="https://akvelon.com/how-to-trim-audio-files-and-add-silence-with-avfoundation-in-swift" target="_blank">Akvelon Blog</a></p>
</blockquote>
<p>While working on one of the apps, we faced the need to trim the recorded audio files. We were working with 32-bit float WAV files, and we had the following requirements:</p>
<ul>
<li>the output file should have the exact same format as the input file;</li>
<li>no processing should be applied to the audio data, audio samples should be copied as-is;</li>
<li>there should be an ability to add silence to the output file.</li>
</ul>
<p>My first guess was to use <code>AVAssetExportSession</code>, but it has limited options for exporting the audio, and it’s not possible to be sure what it does with the audio under the hood. A no-go.</p>
<p>Secondly, I took a look at the requirements again. &ldquo;Audio samples should be copied as-is&rdquo;. That was exactly what we needed - to open the input file for reading, the output one for writing, calculate the range of audio samples to copy, and perform the actual copying. Fortunately, it was completely possible with <code>AVAudioFile</code> - it can be read into <code>AVAudioPCMBuffer</code> and written from one&rsquo;s contents.</p>
<h2 id="a-few-words-regarding-the-output-format">A few words regarding the output format</h2>
<p>While working on implementing this functionality, I was sure that <code>AVFoundation</code> converts the data to the set format of the output file on the fly. However, as later turned out, the header of the output file matched the set format, but the actual audio data had the format of the internal processing with <code>AVFoundation</code> (32-bit float WAV).</p>
<p>It’s possible to use <code>AVAudioConverter</code> to convert the audio to the desired format, but this is out of the scope of this article.</p>
<p>Also, keep in mind that multiple conversions from lossy to lossy format (like MP3 or AAC) will degrade the audio quality. Theoretically, it’s possible to copy the lossy audio data directly, like <a href="https://mpesch3.de" target="_blank">mp3DirectCut</a> does, but this is also a thing to figure out yourself.</p>
<h2 id="preparation">Preparation</h2>
<p>First, we have to open the input file for reading, get some information about it, and open the output file for writing.</p>
<p>Opening the file for reading is simple:</p>
<pre><code class="language-swift">let inputFile: AVAudioFile = try! AVAudioFile(forReading: inputFileURL)
</code></pre>
<blockquote>
<p>Note: All the force unwraps, force tries, and <code>fatalError()</code>’s were added to keep the code examples concise. In the real app, all these things should be handled properly.</p>
</blockquote>
<p>After doing so, we can take a look at 2 important properties of the opened file:</p>
<ul>
<li><code>fileFormat: AVAudioFormat</code> - the format of the audio file itself;</li>
<li><code>processingFormat: AVAudioFormat</code> - the format that AVFoundation will use to process the audio.</li>
</ul>
<p>As I mentioned earlier, the format of our files matched the internal processing format of <code>AVFoundation</code>. That’s why it was possible to use <code>fileFormat</code> for allocating audio buffers and exporting audio data. When I tried an input file with even a slightly different format, everything went down in flames. In the course of the article, I’ll be using <code>processingFormat</code> as the correct one for manipulating audio data.</p>
<p>For our task, we’re going to use the following properties of the input file:</p>
<ul>
<li><code>processingFormat: AVAudioFormat</code> - see above;</li>
<li><code>processingFormat.sampleRate: Double</code> - the sample rate of the audio file;</li>
<li><code>length: Int64</code> - the length of the audio file in samples.</li>
</ul>
<blockquote>
<p>Note: In case digital audio is Greek to you, and you have no clue what the sample rate is, here’s <a href="https://www.izotope.com/en/learn/digital-audio-basics-sample-rate-and-bit-depth.html" target="_blank">a great article describing the basics</a>.</p>
</blockquote>
<p>Later, we&rsquo;re going to need the processing format, its sampling rate, and duration in seconds. Let&rsquo;s define them as the new properties for convenience:</p>
<pre><code class="language-swift">let processingFormat = inputFile.processingFormat
let sampleRate = Int(processingFormat.sampleRate)
let duration = Double(inputFile.length) / Double(sampleRate)
</code></pre>
<blockquote>
<p>Note: In this article, we’re going to cast a lot of values to the different types. At some point, it will become a total mishmash.</p>
</blockquote>
<p>Then we can open the output file for writing, setting it up with the processing format:</p>
<pre><code class="language-swift">let outputFile: AVAudioFile = try! AVAudioFile(
    forWriting: outputFileURL,
    settings: processingFormat.settings,
    commonFormat: processingFormat.commonFormat,
    interleaved: processingFormat.isInterleaved)
</code></pre>
<p>Also, looking a bit ahead, let&rsquo;s define the default buffer size that we&rsquo;re going to use. In my experience, buffer with the size of the sample rate (e.g. storing one second of the audio) works just fine:</p>
<pre><code class="language-swift">let defaultBufferSize = sampleRate
</code></pre>
<p>Finally, we are prepared for the action. It&rsquo;s time to copy some audio data!</p>
<h2 id="copying-the-arbitrary-segment-of-the-audio-file">Copying the arbitrary segment of the audio file</h2>
<p>We&rsquo;ll start with the case where we have to copy a segment of the audio file. Let&rsquo;s define the start and the end time, and check that they are valid:</p>
<pre><code class="language-swift">let startTime: Double = 1.0
let endTime: Double = 2.0

guard
    startTime &lt; endTime,
    startTime &gt;= 0,
    endTime &lt;= duration
else {
    fatalError()
}
</code></pre>
<p>Then we have to figure out the offset of the segment and its duration in samples:</p>
<pre><code class="language-swift">let offset = Int64(Double(sampleRate) * startTime)
var samplesToCopy = Int(Double(sampleRate) * (endTime - startTime))
</code></pre>
<p>Now the interesting part begins. <code>AVAudioFile</code> has <code>framePosition</code> property - &ldquo;the position in the file at which the next read or write operation will occur&rdquo; as the <a href="https://developer.apple.com/documentation/avfaudio/avaudiofile/1390785-frameposition" target="_blank">documentation</a> says. It gets automatically advanced on reading or writing operation by the number of frames read or written respectively. Fortunately for us, it can also be set manually to perform a seek before a read or write. As you might have guessed, this is what we computed the offset for:</p>
<pre><code class="language-swift">inputFile.framePosition = offset
</code></pre>
<p>Finally, it&rsquo;s time to copy the audio data. Let&rsquo;s take a look at the complete code snippet and break it apart right after:</p>
<pre><code class="language-swift">// 1
while samplesToCopy &gt; 0 {
    // 2
    let bufferCapacity = min(samplesToCopy, defaultBufferSize)
    // 3
    let buffer = AVAudioPCMBuffer(
        pcmFormat: processingFormat, 
        frameCapacity: AVAudioFrameCount(bufferCapacity))!

    // 4
    try! inputFile.read(into: buffer)
    try! outputFile.write(from: buffer)
    // 5
    samplesToCopy -= Int(buffer.frameLength)
}
</code></pre>
<ol>
<li>Repeat the process until the whole number of samples is copied;</li>
<li>Define how many samples to copy during the iteration. At some point, the remaining number of samples to copy will be less than a default buffer size, so this should be used as a buffer size instead;</li>
<li>Instantiate the buffer with the proper audio format and the determined size;</li>
<li>Read the input file into the buffer and write it into the output file. Important note: the <a href="https://developer.apple.com/documentation/avfaudio/avaudiofile/1388043-read" target="_blank">documentation</a> says that the system will try to fill the buffer to its capacity, but it&rsquo;s not guaranteed. That&rsquo;s why we&rsquo;re repeating the loop until the desired number of samples is copied rather than copying a precalculated number of the buffers;</li>
<li>Decrease the sample counter by the number of actually copied samples.</li>
</ol>
<p>And the last step - to write the complete output file into the disk, the <code>outputFile</code> has to be deinitialized, which can be achieved by removing all its strong references (if you only referred to it from the property declared inside a method, then it will happen when the method returns). Not as scary as it could be, right?</p>
<h2 id="copying-the-last-part-of-the-audio-file">Copying the last part of the audio file</h2>
<p>Now let&rsquo;s look at how we can cut some corners if we only need to copy the &ldquo;tail&rdquo; of the audio file with the desired length.</p>
<p>This time we&rsquo;ll define a segment duration and check its validity:</p>
<pre><code class="language-swift">let segmentDuration: Double = 4.0
guard segmentDuration &lt;= duration else { fatalError() }
</code></pre>
<p>Offset calculation is also pretty straight-forward:</p>
<pre><code class="language-swift">let offset = inputFile.length - Int64(Double(sampleRate) * segmentDuration)
inputFile.framePosition = offset
</code></pre>
<p>And here lies the main difference - instead of calculating how many samples should be copied, we just perform the copying until we reach the end of the input file:</p>
<pre><code class="language-swift">while inputFile.framePosition &lt; inputFile.length {
    let buffer = AVAudioPCMBuffer(
        pcmFormat: processingFormat, 
        frameCapacity: AVAudioFrameCount(defaultBufferSize))!

    try! inputFile.read(into: buffer)
    try! outputFile.write(from: buffer)
}
</code></pre>
<p>This is one of the practical examples of the situation when the buffer may not be filled completely if there&rsquo;re not enough samples left in the input file.</p>
<p>The rest is identical - deinit the output file and you are free to go!</p>
<h2 id="adding-silence-to-the-output-file">Adding silence to the output file</h2>
<p>Now let&rsquo;s add some silence to the output file, for example, to round up its duration. Obviously, we need a buffer which we will later write into the output file. Let&rsquo;s begin with obtaining one:</p>
<pre><code class="language-swift">let silenceDuration: Double = 0.7
let silenceLength = silenceDuration * Double(sampleRate)

let buffer = AVAudioPCMBuffer(
    pcmFormat: processingFormat, 
    frameCapacity: AVAudioFrameCount(silenceLength))!
</code></pre>
<p>Since in our case we had to add no more than a second of silence, we used a single buffer, but if you consider adding <em>hours</em> of silence, it&rsquo;s better to work with small buffers in a loop similar to what we did earlier.</p>
<p>Now we have an empty buffer, and it has to be filled with silence somehow. The first thing that interests us is the buffer&rsquo;s <code>frameCount</code> property, which represents the number of valid audio frames in the buffer. Its value can be changed manually, so to make the buffer look like it&rsquo;s filled up with data, we can do the following:</p>
<pre><code class="language-swift">buffer.frameLength = buffer.frameCapacity
</code></pre>
<p>If you try to write this buffer into the file now, it <em>may</em> work, especially if you&rsquo;re testing it in a debug build. However, there&rsquo;s no guarantee that the buffer&rsquo;s memory will be zeroed out in the release build, so in the wild, you can get <em>anything</em> but silence.</p>
<p>Therefore, we need to fill the buffer with zeros manually. We&rsquo;ll go a dangerous way, using the old infamous <code>memset</code>. Some smart people <a href="https://pvs-studio.com/en/blog/posts/cpp/0360/" target="_blank">call it</a> the most <a href="https://web.archive.org/web/20170702122030/https://augias.org/paercebal/tech_doc/doc.en/cp.memset_is_evil.html" target="_blank">troublesome</a> function in history, so it wouldn’t hurt to double-check that everything is implemented correctly. This approach was <a href="https://developer.apple.com/forums/thread/24124" target="_blank">described by theanalogkid</a> on the Apple Developer Forums with the sample code written in Objective-C. Here’s how the implementation in Swift code looks:</p>
<pre><code class="language-swift">// 1
let bytesPerFrame = Int(processingFormat.streamDescription.pointee.mBytesPerFrame)
// 2
guard let channelDataPointer = buffer.floatChannelData else { fatalError() }

// 3
for channel in 0..&lt;Int(processingFormat.channelCount) {
    // 4
    memset(channelDataPointer[channel], 0, silenceLenght * bytesPerFrame)
}
</code></pre>
<p>Here&rsquo;s what we do:</p>
<ol>
<li>Getting the size of a single audio frame in bytes;</li>
<li>Getting a pointer to the buffer with the audio data. Since we&rsquo;re working with floating-point audio, the buffer we&rsquo;re looking for is <code>floatChannelData</code>. All the 3 buffer referring properties (float, int16, and int32) are optional, so if you’re unsure about the audio data format, it’s possible to optional chain them like <code>buffer.floatChannelData ?? buffer.int16ChannelData ?? buffer.int32ChannelData</code>;</li>
<li>Iterating through the indexes of the audio channels;</li>
<li>For each audio channel, writing the exact number of zeroed bytes to fill all the audio frames.</li>
</ol>
<p>Now the buffer can be safely written to the file, and that&rsquo;s it. We just added some silence to the audio file. As always, don’t forget to close the output file when you’re done writing into it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As you can see, it’s not that hard to do the basic audio file trimming without using any 3rd-party libraries. The described techniques can be applied further, for example, to compose pieces of the audio file in arbitrary order or to concatenate multiple audio files. The only drawback is the need to convert the audio to the desired output format manually.</p>
    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About me</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/avaudioplayer/">AVAudioPlayer</a>
            </span>
            
            <span>
                <a href="/tags/avfoundation/">AVFoundation</a>
            </span>
            
            <span>
                <a href="/tags/avr/">AVR</a>
            </span>
            
            <span>
                <a href="/tags/c/">C</a>
            </span>
            
            <span>
                <a href="/tags/ctf/">CTF</a>
            </span>
            
            <span>
                <a href="/tags/hackthebox/">HackTheBox</a>
            </span>
            
            <span>
                <a href="/tags/ipod/">iPod</a>
            </span>
            
            <span>
                <a href="/tags/reverse-engineering/">Reverse Engineering</a>
            </span>
            
            <span>
                <a href="/tags/rockbox/">Rockbox</a>
            </span>
            
            <span>
                <a href="/tags/ssd/">SSD</a>
            </span>
            
            <span>
                <a href="/tags/swift/">Swift</a>
            </span>
            
            <span>
                <a href="/tags/uiprogressview/">UIProgressView</a>
            </span>
            
            <span>
                <a href="/tags/uislider/">UISlider</a>
            </span>
            
            <span>
                <a href="/tags/writeup/">Writeup</a>
            </span>
            
            <span>
                <a href="/tags/xbox/">Xbox</a>
            </span>
            
        </div>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/agarmash" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/artem-garmash/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/agarmash_" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://stackoverflow.com/users/7696358/artem-garmash" target="_blank"><span>Stack Overflow</span></a>
            </li>
            
            <li>
                <a href="https://www.strava.com/athletes/47538392" target="_blank"><span>Strava</span></a>
            </li>
            
            <li>
                <a href="https://www.last.fm/user/arch_saint" target="_blank"><span>Last.fm</span></a>
            </li>
            
        </ul>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>TOC</h3>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#a-few-words-regarding-the-output-format">A few words regarding the output format</a></li>
    <li><a href="#preparation">Preparation</a></li>
    <li><a href="#copying-the-arbitrary-segment-of-the-audio-file">Copying the arbitrary segment of the audio file</a></li>
    <li><a href="#copying-the-last-part-of-the-audio-file">Copying the last part of the audio file</a></li>
    <li><a href="#adding-silence-to-the-output-file">Adding silence to the output file</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
    
    
</aside>
        </div>
        <div class="btn">
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            <span>&copy; 2022 <a href="https://agarmash.com">Artem Garmash</a> |
                Powered by <a href="https://gohugo.io/"
                target="_blank">Hugo</a> & modified <a href="https://github.com/amzrk2/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> </span>
        </div>
    </div>
</footer>
    
<script defer src="https://cdn.jsdelivr.net/combine/npm/medium-zoom@1.0.6,npm/lazysizes@5.2.2"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.21.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.21.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>


</body>

</html>